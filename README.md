# Image2TikZ: Knowledge Distillation for DeTikZify

This repository contains the implementation of a knowledge distillation framework for **DeTikZify**, a model that converts raster images (PNG, JPG) into TikZ LaTeX code for generating vector graphics of scientific figures and sketches.

## ğŸ¯ Project Overview

This project implements knowledge distillation techniques to create smaller, faster versions of the DeTikZify model while maintaining high-quality TikZ code generation. The main contributions include:

- **Knowledge Distillation Framework**: A clean implementation for distilling knowledge from larger teacher models to smaller student models
- **Custom Loss Functions**: Specialized distillation losses optimized for sequence-to-sequence generation tasks
- **Evaluation Metrics**: Comprehensive knowledge distillation evaluation suite with KL divergence, confidence alignment, and prediction quality metrics
- **Training Pipeline**: Simplified training scripts with checkpoint management

## ğŸ—ï¸ Architecture

The project is built on top of the original [DeTikZify](https://github.com/potamides/DeTikZify) framework and includes:

- **Teacher Model**: Large pre-trained DeTikZify model (e.g., `nllg/detikzify-cl-7b`)
- **Student Model**: Smaller model to be trained via distillation (e.g., `nllg/detikzify-tl-1.1b`)
- **Knowledge Distillation**: Logits-based distillation with temperature scaling and loss balancing

## ğŸ“ Repository Structure

```
â”œâ”€â”€ detikzify/                  # Core DeTikZify framework
â”‚   â”œâ”€â”€ model/                  # Model architectures and loading utilities
â”‚   â”œâ”€â”€ infer/                  # Inference pipeline and TikZ generation
â”‚   â”œâ”€â”€ evaluate/               # Evaluation metrics and benchmarks
â”‚   â”œâ”€â”€ dataset/                # Dataset loading and preprocessing
â”‚   â””â”€â”€ train/                  # Training utilities
â”œâ”€â”€ evaluation/                 # Modular evaluation framework
â”‚   â”œâ”€â”€ core_metrics.py         # Core KL and distillation metrics
â”‚   â”œâ”€â”€ advanced_analysis.py    # Top-K and confidence analysis
â”‚   â”œâ”€â”€ eval_utils.py          # Model loading and data utilities
â”‚   â”œâ”€â”€ evaluator.py           # Main evaluation orchestrator
â”‚   â””â”€â”€ run_evaluation.py      # CLI evaluation interface
â”œâ”€â”€ train.py                   # Main training script for knowledge distillation
â”œâ”€â”€ inference.py               # Simple inference example
â”œâ”€â”€ model_utils.py             # Model loading and management utilities
â”œâ”€â”€ data_utils.py              # Data processing for knowledge distillation
â”œâ”€â”€ training_utils.py          # Training step implementations
â”œâ”€â”€ loss_functions.py          # Custom loss functions for distillation
â”œâ”€â”€ checkpoint_utils.py        # Checkpoint saving and loading
â””â”€â”€ requirements.txt           # Python dependencies
```

## ğŸš€ Getting Started

### Prerequisites

1. **Python Environment**: Python 3.8+
2. **LaTeX Distribution**: TeX Live 2023+ with pdflatex, lualatex, xelatex
3. **System Dependencies**:
   - Ghostscript (for PDF processing)
   - Poppler utils (for PDF to image conversion)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/younisraafat22/Image2TikZ.git
cd Image2TikZ
```

2. Install Python dependencies:
```bash
pip install -r requirements.txt
```

3. Install system dependencies:

**Option A: Using conda (recommended - no sudo required):**
```bash
conda install -c conda-forge texlive-core ghostscript poppler
```

**Option B: Using system package manager (Ubuntu/Debian):**
```bash
sudo apt-get update
sudo apt-get install texlive-full ghostscript poppler-utils
```

## ğŸ“Š Usage

### Knowledge Distillation Training

Train a student model using knowledge distillation:

```bash
python train.py \\
    --output_dir ./outputs/my_distilled_model \\
    --teacher_model nllg/detikzify-cl-7b \\
    --student_model nllg/detikzify-tl-1.1b \\
    --dataset nllg/datikz-v3 \\
    --num_train_epochs 2 \\
    --batch_size 1 \\
    --gradient_accumulation_steps 32 \\
    --learning_rate 1e-5 \\
    --temperature 4.0 \\
    --alpha 0.2
```

### Simple Inference

Generate TikZ code from an image using a trained model:

```python
from inference import load_distilled_model
from detikzify.infer import DetikzifyPipeline
from PIL import Image

# Load your trained model
model, tokenizer = load_distilled_model("path/to/checkpoint", device="cuda")

# Create pipeline
pipeline = DetikzifyPipeline(model=model, tokenizer=tokenizer)

# Generate TikZ code
image = Image.open("your_image.png")
tikz_doc = pipeline.sample(image=image)

print("Generated TikZ code:")
print(tikz_doc.code)

# Save as PDF if compilation successful
if tikz_doc.pdf:
    tikz_doc.save("output.pdf")
```

### Evaluation

Evaluate knowledge distillation performance using the modular evaluation framework:

```bash
cd evaluation
python run_evaluation.py \\
    --teacher_model nllg/detikzify-cl-7b \\
    --distilled_model ../outputs/my_distilled_model/checkpoint-best \\
    --dataset nllg/datikz-v3 \\
    --compare_all \\
    --output_file evaluation_results.json \\
    --max_samples 1000 \\
    --temperature 3.0
```

## ğŸ“ˆ Key Features

### Knowledge Distillation
- **Temperature Scaling**: Softens teacher logits for better knowledge transfer
- **Loss Balancing**: Combines distillation loss with task-specific cross-entropy loss
- **Gradient Accumulation**: Enables training with larger effective batch sizes

### Evaluation Metrics
- **KL Divergence**: Measures knowledge transfer quality from teacher to student
- **Jensen-Shannon Divergence**: Symmetric measure of distribution similarity
- **Confidence Alignment**: How well student confidence matches teacher confidence
- **Top-K Analysis**: Overlap between teacher and student top predictions
- **Rank Correlation**: Spearman correlation of probability rankings
- **Calibration**: Expected Calibration Error (ECE) for prediction reliability

### Evaluation Framework
- **Modular Design**: Clean separation of metrics, analysis, and utilities
- **Core Metrics**: ECE, JS divergence, rank correlation, entropy analysis
- **Advanced Analysis**: Confidence alignment, top-K quality, prediction diversity
- **Comprehensive Comparison**: Teacher, baseline student, and distilled model evaluation

### Training Features
- **Checkpoint Management**: Automatic saving and loading of best models
- **Console Logging**: Training progress and evaluation metrics tracking
- **Mixed Precision**: Efficient training with bfloat16 precision
- **Gradient Clipping**: Stable training with gradient norm clipping

## ğŸ”¬ Research Context

This implementation is part of a Master's thesis exploring knowledge distillation techniques for multimodal sequence-to-sequence models. The work focuses on:

1. **Model Compression**: Reducing model size while maintaining performance
2. **Efficiency Optimization**: Faster inference for practical deployment
3. **Quality Preservation**: Maintaining high-quality TikZ code generation

## ğŸ“š Citation

If you use this code in your research, please cite the original DeTikZify paper:

```bibtex
@misc{belouadi2024detikzify,
    title={DeTi{k}Zify: Synthesizing Graphics Programs for Scientific Figures and Sketches with Ti{k}Z}, 
    author={Jonas Belouadi and Simone Paolo Ponzetto and Steffen Eger},
    year={2024},
    eprint={2405.15306},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

## ğŸ“ License

This project builds upon the [DeTikZify](https://github.com/potamides/DeTikZify) framework. Please refer to the original repository for licensing information.

The MCTS implementation is licensed under the MIT License (see `detikzify/mcts/LICENSE`).

## ğŸ”— Related Resources

- [Original DeTikZify Repository](https://github.com/potamides/DeTikZify)
- [DeTikZify Paper](https://arxiv.org/abs/2405.15306)
- [TikZ Documentation](https://ctan.org/pkg/pgf)
- [Hugging Face Models](https://huggingface.co/nllg)

## ğŸ› ï¸ Troubleshooting

- Ensure LaTeX is properly installed and accessible via PATH
- For compilation errors, check that all required LaTeX packages are installed
- If running out of GPU memory, reduce batch size or use gradient checkpointing